{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e12593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx = 792.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# for the equation: ğ‘¦ = 3(ğ‘¡^2 + 2)^2 ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ ğ‘¡ = 2ğ‘¥ + c using autograd;\n",
    "# Initialize x and c as tensors with the value 1\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "c = torch.tensor(1.0, requires_grad=False)\n",
    "\n",
    "# Define t and y according to the given equations\n",
    "t = 2*x + c\n",
    "y = 3*(t**2 + 2)**2\n",
    "\n",
    "# Compute the gradient\n",
    "y.backward()\n",
    "\n",
    "# Access the gradient of x\n",
    "dx = x.grad\n",
    "\n",
    "print(f\"dy/dx = {dx.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b81cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx = 78.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# for the equation: ğ‘¦ = 3(ğ‘ ^3 + ğ‘ ) + 2ğ‘^4, ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ ğ‘  = 2x\n",
    "\n",
    "# Initialize x as a tensor with the value 1\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# c is a constant, so it's defined as a Python variable or a tensor without requires_grad\n",
    "c = 1.0\n",
    "\n",
    "# Define s and y according to the given equations\n",
    "s = 2*x\n",
    "y = 3*(s**3 + s) + 2*(c**4)\n",
    "\n",
    "# Compute the gradient\n",
    "y.backward()\n",
    "\n",
    "# Access the gradient of x\n",
    "dx = x.grad\n",
    "\n",
    "print(f\"dy/dx = {dx.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56eb5ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx = 51347072221184.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# for the equation: ğ‘¦ = 2ğ‘¡ + ğ‘, ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ ğ‘¡ = (ğ‘^2 + 2ğ‘ + 3)^2, ğ‘ = 2ğ‘Ÿ^3 + 3ğ‘Ÿ, ğ‘Ÿ = 2ğ‘ + 3, ğ‘ = 2ğ‘¥ + c\n",
    "# Initialize x as a tensor with the value 1 and requires_grad=True\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# Since c is a constant, it can be defined as a Python variable or a tensor without requires_grad\n",
    "c = 1.0\n",
    "\n",
    "# Define q, r, p, t, and y according to the given equations\n",
    "q = 2*x + c\n",
    "r = 2*q + 3\n",
    "p = 2*r**3 + 3*r\n",
    "t = (p**2 + 2*p + 3)**2\n",
    "y = 2*t + c\n",
    "\n",
    "# Compute the gradient\n",
    "y.backward()\n",
    "\n",
    "# Access the gradient of x\n",
    "dx = x.grad\n",
    "\n",
    "print(f\"dy/dx = {dx.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b6b452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 21.711444854736328\n",
      "Epoch 101, Loss: 1.2933391332626343\n",
      "Epoch 201, Loss: 0.16098171472549438\n",
      "Epoch 301, Loss: 0.027906939387321472\n",
      "Epoch 401, Loss: 0.006669664289802313\n",
      "Epoch 501, Loss: 0.002830569865182042\n",
      "Epoch 601, Loss: 0.0021082102321088314\n",
      "Epoch 701, Loss: 0.0019705696031451225\n",
      "Epoch 801, Loss: 0.0019442342454567552\n",
      "Epoch 901, Loss: 0.0019391851965337992\n",
      "Final a: 0.6887469291687012, b: 4.952706336975098\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#Part 2\n",
    "# for the model: ğ‘“(ğ‘¥) = ğ‘’^âˆ’ğ‘ğ‘¥ + 2ğ‘ğ‘¥ + ğ‘\n",
    "# Data\n",
    "X = torch.tensor([-2, -1.9, -1.8, -1.7, -1.6, -1.5, -1.4, -1.3, -1.2, -1.1,\n",
    "                  -1, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0,\n",
    "                  0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1,\n",
    "                  1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0], dtype=torch.float32)\n",
    "\n",
    "y0 = torch.tensor([6.255, 6.121, 6.005, 5.907, 5.825, 5.758, 5.704, 5.664, 5.636, 5.62,\n",
    "                   5.614, 5.618, 5.631, 5.652, 5.682, 5.719, 5.763, 5.814, 5.87, 5.933,\n",
    "                   6.0, 6.072, 6.149, 6.231, 6.316, 6.405, 6.497, 6.593, 6.691, 6.793,\n",
    "                   6.897, 7.003, 7.112, 7.223, 7.335, 7.45, 7.566, 7.684, 7.804, 7.924], dtype=torch.float32)\n",
    "\n",
    "# Initialize parameters a and b\n",
    "a = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# Number of iterations\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Model prediction\n",
    "    y_pred = torch.exp(-a * X) + 2 * a * X + b\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = ((y_pred - y0) ** 2).mean()\n",
    "    \n",
    "    # Zero gradients\n",
    "    if a.grad is not None:\n",
    "        a.grad.zero_()\n",
    "    if b.grad is not None:\n",
    "        b.grad.zero_()\n",
    "    \n",
    "    # Perform backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        a -= lr * a.grad\n",
    "        b -= lr * b.grad\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "print(f'Final a: {a.item()}, b: {b.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f56f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
